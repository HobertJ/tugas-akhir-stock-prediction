{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP8fGriseyQB"
      },
      "source": [
        "# Overview\n",
        "\n",
        "This notebook contain the relevance score experiment for the study\n",
        "\n",
        "- Dataset that is used is the 9 years stock data that has been formed\n",
        "- Model that is used in the study leverage an ensemble learning architecture, using LSTM, GRU, and Transformer as base learners, and Multi Layer Perceptron (MLP) as meta learner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljn4TF9iR5ge"
      },
      "source": [
        "# Import and Define Constant"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q87jwHb5GqC2",
        "outputId": "a13a630f-ab39-4225-fc24-6001a6e02b00"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, mean_absolute_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Input, Dropout, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.regularizers import L2\n",
        "from matplotlib.ticker import MultipleLocator\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# DEFINE CONSTANT\n",
        "\n",
        "# constant for train size, the rest of the data is used for test\n",
        "TRAIN_SIZE = 0.85\n",
        "\n",
        "N_COMPONENT_PCA = 0.8\n",
        "TARGET_COLUMN_NAME = \"Closing Price\"\n",
        "\n",
        "# VARIABLE IN THIS EXPERIMENT\n",
        "RELEVANCE_COLUMN_USED = \"naive\"\n",
        "# RELEVANCE_COLUMN_USED = \"relevance_score_linear\"\n",
        "# RELEVANCE_COLUMN_USED = \"relevance_score_exponential\"\n",
        "# RELEVANCE_COLUMN_USED = \"relevance_score_logarithmic\"\n",
        "\n",
        "# variasi sentiment\n",
        "EMBEDDING_COLUMN_NAME = \"text_embedding_multilingual_mpnet\"\n",
        "USE_PCA = True\n",
        "\n",
        "LEARNING_RATE_LSTM_GRU = 0.001\n",
        "N_ITERATION = 20\n",
        "CONTEXT_WINDOW = 5\n",
        "NUM_EPOCHS = 100\n",
        "LOSS = \"mse\"\n",
        "METRICS = \"mape\"\n",
        "\n",
        "# constant for transformer model\n",
        "NUM_TRANSFORMER_HEADS = 4\n",
        "NUM_TRANSFORMER_LAYERS = 2\n",
        "NUM_TRANSFORMER_HIDDEN_DIM = 32\n",
        "LEARNING_RATE_TRANSFORMER = 0.0001\n",
        "\n",
        "fundamental_features  = [\n",
        "  \"Financing Cash Flow\",\n",
        "  \"P/B Ratio\",\n",
        "  \"P/S Ratio\",\n",
        "  \"Capital Adequacy Ratio\",\n",
        "  \"Debt to Assets Ratio\",\n",
        "  \"Debt to Equity Ratio\",\n",
        "  \"Investing Cash Flow\",\n",
        "  \"Operating Cash Flow\",\n",
        "  \"Return on Assets\",\n",
        "  \"Operating Profit\",\n",
        "  \"Loan to Deposit Ratio\"\n",
        "]\n",
        "\n",
        "historic_price_features = [\n",
        "    'Opening Price',\n",
        "    'Highest Price',\n",
        "    'Lowest Price',\n",
        "    'Volume',\n",
        "    'Change'\n",
        "]\n",
        "\n",
        "relevance_score_features = [\n",
        "    'relevance_score_linear',\n",
        "    'relevance_score_logarithmic',\n",
        "    'relevance_score_exponential'\n",
        "]\n",
        "\n",
        "print(f\"Number of fundamental features: {len(fundamental_features)}\")\n",
        "print(f\"Number of historic price features: {len(historic_price_features)}\")\n",
        "print(f\"Number of relevance score variance: {len(relevance_score_features)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNO-_BfISAWo"
      },
      "source": [
        "# Define Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WmK6K_cSBnm"
      },
      "source": [
        "## Parse Embedding From Pandas Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNzDdwCdFbfu"
      },
      "outputs": [],
      "source": [
        "def parse_embedding_from_df(df, embedding_column_name, n_comp, target_column_name=\"Closing Price\"):\n",
        "\n",
        "  feature_columns = fundamental_features  + historic_price_features + relevance_score_features\n",
        "\n",
        "  column = [target_column_name, embedding_column_name] + feature_columns\n",
        "  df_processed = df[column].copy()\n",
        "\n",
        "  def parse_embedding_string(embedding_str):\n",
        "\n",
        "      if not isinstance(embedding_str, str):\n",
        "          return None # Handle non-string inputs (like NaN)\n",
        "\n",
        "      # Remove brackets and split by whitespace\n",
        "      embedding_str = embedding_str.strip().strip('[]')\n",
        "      # Use regex to find all floating point numbers, including those in scientific notation\n",
        "      numbers = re.findall(r\"[-+]?\\d*\\.?\\d+[eE][-+]?\\d+|[-+]?\\d*\\.\\d+|\\d+\", embedding_str)\n",
        "\n",
        "      try:\n",
        "          # Convert the extracted numbers to floats\n",
        "          return [float(num) for num in numbers]\n",
        "      except ValueError:\n",
        "          return None # Return None if conversion to float fails for any number\n",
        "\n",
        "  # Apply the parsing function to the embedding column and handle potential None values\n",
        "  df_processed['embedding'] = df_processed[embedding_column_name].apply(parse_embedding_string)\n",
        "\n",
        "  # Handle rows where parsing failed (e.g., by filling with zeros)\n",
        "  # Determine the embedding dimension from the first successfully parsed embedding\n",
        "  embedding_dim = None\n",
        "  for embedding_list in df_processed['embedding']:\n",
        "      if embedding_list is not None:\n",
        "          embedding_dim = len(embedding_list)\n",
        "          break\n",
        "\n",
        "  if embedding_dim is None:\n",
        "      # Handle case where all embeddings are None or invalid\n",
        "      # Using a default BERT base dimension as a fallback.\n",
        "      embedding_dim = 768 # Default BERT base dimension\n",
        "      print(f\"Warning: Could not determine embedding dimension from data. Using a default embedding dimension of {embedding_dim}.\")\n",
        "\n",
        "  df_processed['embedding'] = df_processed['embedding'].apply(lambda x: np.array(x, dtype=float) if x is not None else np.zeros(embedding_dim))\n",
        "\n",
        "  def pca_reduce(embedding_list, n_comp=n_comp):\n",
        "    old_length = len(embedding_list)\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    embedding_reduced = pca.fit_transform(np.array(embedding_list.tolist())) # Convert list of arrays to numpy array\n",
        "    print(f\"Old length of embedding: {old_length}\")\n",
        "    print(f\"Number of components to explain {n_comp*100}% variance: {pca.n_components_}\")\n",
        "    return embedding_reduced\n",
        "\n",
        "  df_processed['embedding_pca'] = pca_reduce(df_processed['embedding']).tolist()\n",
        "\n",
        "  # Convert feature columns to numeric, coercing errors, and fill NaNs\n",
        "  for col in feature_columns:\n",
        "      df_processed[col] = pd.to_numeric(df_processed[col], errors='coerce')\n",
        "\n",
        "  return df_processed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqmfig3wSHVs"
      },
      "source": [
        "## Create the Train, Test, and Val Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6uvNsreGoCx"
      },
      "outputs": [],
      "source": [
        "def create_data(df_processed, use_pca, embedding_column_name, relevance_column, target_column_price=\"Closing Price\", context_window=5):\n",
        "\n",
        "    def create_sequences(data, target_index, context_window):\n",
        "      X, y = [], []\n",
        "      for i in range(len(data) - context_window):\n",
        "        X.append(data[i:i+context_window, :])\n",
        "        y.append(data[i+context_window, target_index])\n",
        "      return np.array(X), np.array(y)\n",
        "\n",
        "    # Separate target variable and features\n",
        "    target_data = df_processed[target_column_price].values\n",
        "\n",
        "    # exclude irrelevant column from the feature_list\n",
        "    relevance_columns = ['relevance_score_linear', 'relevance_score_exponential', 'relevance_score_logarithmic']\n",
        "\n",
        "    if RELEVANCE_COLUMN_USED != 'naive':\n",
        "      relevance_columns.remove(RELEVANCE_COLUMN_USED)\n",
        "\n",
        "    feature_columns = [col for col in df_processed.columns if col not in ([target_column_price, embedding_column_name, 'Date', 'embedding', 'embedding_pca'] + relevance_columns)]\n",
        "\n",
        "    feature_data = df_processed[feature_columns].values\n",
        "\n",
        "    if use_pca:\n",
        "      embedding_data = np.array(df_processed['embedding_pca'].tolist())\n",
        "    else:\n",
        "      embedding_data = np.array(df_processed['embedding'].tolist())\n",
        "\n",
        "    print(\"Feature used in the dataset other than sentiment: \")\n",
        "    for index, col in enumerate(feature_columns):\n",
        "      print(f\"{index+1}. {col}\")\n",
        "\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_target_data = scaler.fit_transform(target_data.reshape(-1, 1))\n",
        "\n",
        "    feature_scaler = MinMaxScaler()\n",
        "    scaled_feature_data = feature_scaler.fit_transform(feature_data)\n",
        "\n",
        "    combined_data = np.concatenate((scaled_target_data, embedding_data, scaled_feature_data), axis=1)\n",
        "\n",
        "    # Calculate split indices\n",
        "    n_total = len(combined_data) - CONTEXT_WINDOW\n",
        "    train_split_index = int(n_total * TRAIN_SIZE)\n",
        "\n",
        "    train_data = combined_data[:train_split_index + CONTEXT_WINDOW]\n",
        "    test_data = combined_data[train_split_index:]\n",
        "\n",
        "    train_generator = TimeseriesGenerator(train_data, train_data[:, 0], # Target is the first column (scaled 'close')\n",
        "                                        length=CONTEXT_WINDOW, batch_size=24)\n",
        "\n",
        "    test_generator = TimeseriesGenerator(test_data, test_data[:, 0], # Target is the first column (scaled 'close')\n",
        "                                      length=CONTEXT_WINDOW, batch_size=24)\n",
        "\n",
        "    X, y = create_sequences(combined_data, target_index=0, context_window=context_window)\n",
        "\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
        "\n",
        "    X_train, X_test = X_tensor[:train_split_index], X_tensor[train_split_index:]\n",
        "    y_train, y_test = y_tensor[:train_split_index], y_tensor[train_split_index:]\n",
        "\n",
        "    train_dataset = TensorDataset(X_train, y_train)\n",
        "    test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    return train_generator, test_generator, scaler, combined_data, train_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxzqQ_thSOFr"
      },
      "source": [
        "## Create GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L0WaHcQFJQUG"
      },
      "outputs": [],
      "source": [
        "def create_gru_model(combined_data, print_summary=True):\n",
        "\n",
        "  model = Sequential([\n",
        "      Input(shape=(CONTEXT_WINDOW, combined_data.shape[1])),\n",
        "      GRU(units=64,\n",
        "          activation='relu',\n",
        "          return_sequences=True,\n",
        "          kernel_regularizer=L2(0.001)\n",
        "      ),\n",
        "      GRU(units=32,\n",
        "          activation='relu',\n",
        "          recurrent_dropout=0.25\n",
        "      ),\n",
        "      Dense(32, activation='relu'),\n",
        "      Dense(1, activation='linear')\n",
        "  ])\n",
        "\n",
        "  OPTIMIZER = Adam(learning_rate=LEARNING_RATE_LSTM_GRU)\n",
        "  model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[METRICS])\n",
        "\n",
        "  if print_summary:\n",
        "    model.summary()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Slj0MtnfSQXl"
      },
      "source": [
        "## Create LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPWmlElAJZAj"
      },
      "outputs": [],
      "source": [
        "def create_lstm_model(combined_data, print_summary=True):\n",
        "\n",
        "  model = Sequential([\n",
        "    Input(shape=(CONTEXT_WINDOW, combined_data.shape[1])),\n",
        "    LSTM(units=64,\n",
        "          activation='relu',\n",
        "          return_sequences=True,\n",
        "          kernel_regularizer=L2(0.001),\n",
        "    ),\n",
        "    LSTM(units=32,\n",
        "          activation='relu',\n",
        "          recurrent_dropout=0.25\n",
        "    ),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='linear')\n",
        "  ])\n",
        "\n",
        "  OPTIMIZER = Adam(learning_rate=LEARNING_RATE_LSTM_GRU)\n",
        "  model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[METRICS])\n",
        "\n",
        "  if print_summary:\n",
        "    model.summary()\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G5swda2SeDl"
      },
      "source": [
        "## Create Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sauXrCTSdpD"
      },
      "outputs": [],
      "source": [
        "def create_transformer_model(combined_data, train_loader):\n",
        "\n",
        "  class TimeSeriesTransformer(nn.Module):\n",
        "    def __init__(self, feature_size, num_heads=NUM_TRANSFORMER_HEADS, num_layers=NUM_TRANSFORMER_LAYERS, hidden_dim=NUM_TRANSFORMER_HIDDEN_DIM):\n",
        "      super().__init__()\n",
        "      self.input_proj = nn.Linear(feature_size, hidden_dim)\n",
        "      encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dim_feedforward=2*hidden_dim, batch_first=True)\n",
        "      self.transformer = nn.TransformerEncoder(encoder_layer=encoder_layer, num_layers=num_layers)\n",
        "      self.regressor = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.input_proj(x)\n",
        "      x = self.transformer(x)\n",
        "      return self.regressor(x[:, -1, :]).squeeze()\n",
        "\n",
        "  feature_size = combined_data.shape[1]\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  transformer_model = TimeSeriesTransformer(feature_size=feature_size).to(device)\n",
        "  criterion = nn.MSELoss()\n",
        "  optimizer = torch.optim.Adam(transformer_model.parameters(), lr=LEARNING_RATE_TRANSFORMER)\n",
        "\n",
        "  return transformer_model, criterion, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P16NCacQVpSM"
      },
      "source": [
        "## Define Second Layer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY480JeeVozf"
      },
      "outputs": [],
      "source": [
        "def create_second_layer_model(input_data_second_layer):\n",
        "  second_layer_model = Sequential([\n",
        "      Input(shape=(input_data_second_layer.shape[1],)),\n",
        "      Dense(10, activation='relu'),\n",
        "      Dense(5, activation='relu'),\n",
        "      Dense(1)\n",
        "  ])\n",
        "\n",
        "  OPTIMIZER = Adam(learning_rate=LEARNING_RATE_LSTM_GRU)\n",
        "  second_layer_model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=[METRICS])\n",
        "\n",
        "  return second_layer_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59ye_d0fUE7M"
      },
      "source": [
        "# Load the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDNLCv7dJ1E2",
        "outputId": "7c0e5eb0-6b01-47be-ad60-67f739957ab3"
      },
      "outputs": [],
      "source": [
        "final_df = pd.read_csv(\"final_bmri_dataset.csv\")\n",
        "# final_df = pd.read_csv(\"final_bbri_dataset.csv\")\n",
        "# final_df = pd.read_csv(\"final_bbca_dataset.csv\")\n",
        "\n",
        "df_processed = parse_embedding_from_df(final_df, EMBEDDING_COLUMN_NAME, n_comp=N_COMPONENT_PCA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PqEWwdsULR8"
      },
      "source": [
        "# Create Data for Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ6PjnX1J3Hj",
        "outputId": "5ca24c32-5e7d-4f60-fd9f-9d7f770e2d75"
      },
      "outputs": [],
      "source": [
        "train_generator, test_generator, scaler, combined_data, train_loader, test_loader = create_data(\n",
        "    df_processed=df_processed,\n",
        "    use_pca=USE_PCA,\n",
        "    embedding_column_name=EMBEDDING_COLUMN_NAME,\n",
        "    relevance_column = RELEVANCE_COLUMN_USED\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "b467ac1ced994667b50b1488396f1c9e",
            "f67ee036ec6c49208caf7bfbd0eb99cf",
            "059a81282cba4b81a5b4908f3f5b5df8",
            "8ef3b92b8adc4154b7df0542f469172c",
            "1469ea1f14f643bab8e80fdf6460adec",
            "466506bbf67f408f810f3dffce1e82f2",
            "e5463e034da14e2fa2bda9144cd80931",
            "c0d909e6102248a0984758b927aaf603",
            "4b19bb8a52f74125b27a2d26974b8c48",
            "485d711c8a53483481865e7c931bbd20",
            "170f3674827047d0b34f37529e73b9f8"
          ]
        },
        "id": "yqlZwl75PYCe",
        "outputId": "72ab1dda-508b-4e7b-9225-33f94d44c934"
      },
      "outputs": [],
      "source": [
        "# lstm error\n",
        "sum_mse_lstm = 0\n",
        "sum_mae_lstm = 0\n",
        "sum_mape_lstm = 0\n",
        "\n",
        "# gru error\n",
        "sum_mse_gru = 0\n",
        "sum_mae_gru = 0\n",
        "sum_mape_gru = 0\n",
        "\n",
        "# transformer error\n",
        "sum_mse_transformer = 0\n",
        "sum_mae_transformer = 0\n",
        "sum_mape_transformer = 0\n",
        "\n",
        "# second layer eval\n",
        "sum_mse_second_layer = 0\n",
        "sum_mae_second_layer = 0\n",
        "sum_mape_second_layer = 0\n",
        "\n",
        "for i in tqdm(range(N_ITERATION), desc='training and evaluating model'):\n",
        "\n",
        "  # create and train the LSTM model\n",
        "  lstm_model = create_lstm_model(combined_data, print_summary=False)\n",
        "\n",
        "  early_stopping = EarlyStopping(\n",
        "      monitor='loss', # Monitor train loss\n",
        "      patience=10,        # Number of epochs with no improvement after which training will be stopped.\n",
        "      restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity.\n",
        "  )\n",
        "\n",
        "  history = lstm_model.fit(\n",
        "      train_generator,\n",
        "      epochs=NUM_EPOCHS,\n",
        "      verbose=0,\n",
        "      callbacks=[early_stopping] # Add the early stopping callback here\n",
        "  )\n",
        "\n",
        "  # create and train the GRU model\n",
        "  gru_model = create_gru_model(combined_data, print_summary=False)\n",
        "\n",
        "  early_stopping = EarlyStopping(\n",
        "      monitor='loss', # Monitor train loss\n",
        "      patience=10,        # Number of epochs with no improvement after which training will be stopped.\n",
        "      restore_best_weights=True # Restore model weights from the epoch with the best value of the monitored quantity.\n",
        "  )\n",
        "\n",
        "  history = gru_model.fit(\n",
        "      train_generator,\n",
        "      epochs=NUM_EPOCHS,\n",
        "      verbose=0,\n",
        "      callbacks=[early_stopping] # Add the early stopping callback here\n",
        "  )\n",
        "\n",
        "  transformer_model, criterion, optimizer = create_transformer_model(combined_data, train_loader)\n",
        "\n",
        "  # Check for GPU availability and set the device\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "  # model training\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "    transformer_model.train()\n",
        "    total_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "      X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      outputs = transformer_model(X_batch)\n",
        "      loss = criterion(outputs, y_batch)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  actual_train_data = np.array(train_generator.targets[CONTEXT_WINDOW:])\n",
        "\n",
        "  # making prediction for second layer training data\n",
        "  lstm_train_predictions = lstm_model.predict(train_generator, verbose=0)\n",
        "  gru_train_predictions = gru_model.predict(train_generator, verbose=0)\n",
        "\n",
        "  transformer_model.eval()\n",
        "  transformer_train_predictions = []\n",
        "  with torch.no_grad():\n",
        "    for X_batch, y_batch in train_loader:\n",
        "      X_batch = X_batch.to(device)\n",
        "      outputs = transformer_model(X_batch)\n",
        "      transformer_train_predictions.extend(outputs.cpu().numpy())\n",
        "\n",
        "  transformer_train_predictions = np.array(transformer_train_predictions).reshape(-1, 1)\n",
        "\n",
        "  # concatenate all the train prediction data to form the complete training data for second layer model\n",
        "  predicted_train = np.concatenate((lstm_train_predictions, gru_train_predictions, transformer_train_predictions), axis=1)\n",
        "\n",
        "  # create second layer model\n",
        "  second_layer_model = create_second_layer_model(predicted_train)\n",
        "\n",
        "  # train second layer model\n",
        "  second_layer_model.fit(predicted_train, actual_train_data, epochs=NUM_EPOCHS, verbose=0)\n",
        "\n",
        "  # making prediction on test data using lstm and gru\n",
        "  lstm_test_predictions = lstm_model.predict(test_generator, verbose=0)\n",
        "  gru_test_predictions = gru_model.predict(test_generator, verbose=0)\n",
        "\n",
        "  # making prediction on test data using transformer\n",
        "  transformer_model.eval()\n",
        "  transformer_test_predictions, trues = [], []\n",
        "  with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "      X_batch, y_batch = X_batch.to(device), y_batch.to(device) # Move data to device\n",
        "      outputs = transformer_model(X_batch)\n",
        "      transformer_test_predictions.extend(outputs.cpu().numpy()) # Move predictions back to CPU for evaluation\n",
        "      trues.extend(y_batch.cpu().numpy()) # Move true values back to CPU for evaluation\n",
        "\n",
        "  transformer_test_predictions = np.array(transformer_test_predictions).reshape(-1, 1)\n",
        "\n",
        "  # concatenate the predictions from three base model to form single dataset\n",
        "  predicted_test = np.concatenate((lstm_test_predictions, gru_test_predictions, transformer_test_predictions), axis=1)\n",
        "\n",
        "  lstm_test_predictions_actual = scaler.inverse_transform(lstm_test_predictions)\n",
        "  gru_test_predictions_actual = scaler.inverse_transform(gru_test_predictions)\n",
        "  transformer_test_predictions_actual = scaler.inverse_transform(transformer_test_predictions)\n",
        "\n",
        "  second_layer_test_predictions = second_layer_model.predict(predicted_test, verbose=0)\n",
        "\n",
        "  predicted_actual = scaler.inverse_transform(second_layer_test_predictions)\n",
        "  true_actual = scaler.inverse_transform(test_generator.targets.reshape(-1, 1))\n",
        "\n",
        "  # trim the true actual to be the same length\n",
        "  true_actual = true_actual[CONTEXT_WINDOW:]\n",
        "\n",
        "  # lstm eval\n",
        "  sum_mse_lstm += mean_squared_error(true_actual, lstm_test_predictions_actual)\n",
        "  sum_mae_lstm += mean_absolute_error(true_actual, lstm_test_predictions_actual)\n",
        "  sum_mape_lstm += mean_absolute_percentage_error(true_actual, lstm_test_predictions_actual)\n",
        "\n",
        "  # gru eval\n",
        "  sum_mse_gru += mean_squared_error(true_actual, gru_test_predictions_actual)\n",
        "  sum_mae_gru += mean_absolute_error(true_actual, gru_test_predictions_actual)\n",
        "  sum_mape_gru += mean_absolute_percentage_error(true_actual, gru_test_predictions_actual)\n",
        "\n",
        "  # transformer eval\n",
        "  sum_mse_transformer += mean_squared_error(true_actual, transformer_test_predictions_actual)\n",
        "  sum_mae_transformer += mean_absolute_error(true_actual, transformer_test_predictions_actual)\n",
        "  sum_mape_transformer += mean_absolute_percentage_error(true_actual, transformer_test_predictions_actual)\n",
        "\n",
        "  # second layer eval\n",
        "  sum_mse_second_layer += mean_squared_error(true_actual, predicted_actual)\n",
        "  sum_mae_second_layer += mean_absolute_error(true_actual, predicted_actual)\n",
        "  sum_mape_second_layer += mean_absolute_percentage_error(true_actual, predicted_actual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFgzECGVadbo",
        "outputId": "3960a906-aabc-4e5f-f335-8db627ac3b6a"
      },
      "outputs": [],
      "source": [
        "average_mse_lstm = sum_mse_lstm / N_ITERATION\n",
        "average_mae_lstm = sum_mae_lstm / N_ITERATION\n",
        "average_mape_lstm = sum_mape_lstm / N_ITERATION\n",
        "\n",
        "average_mse_gru = sum_mse_gru / N_ITERATION\n",
        "average_mae_gru = sum_mae_gru / N_ITERATION\n",
        "average_mape_gru = sum_mape_gru / N_ITERATION\n",
        "\n",
        "average_mse_transformer = sum_mse_transformer / N_ITERATION\n",
        "average_mae_transformer = sum_mae_transformer / N_ITERATION\n",
        "average_mape_transformer = sum_mape_transformer / N_ITERATION\n",
        "\n",
        "average_mse_second_layer = sum_mse_second_layer / N_ITERATION\n",
        "average_mae_second_layer = sum_mae_second_layer / N_ITERATION\n",
        "average_mape_second_layer = sum_mape_second_layer / N_ITERATION\n",
        "\n",
        "print(\"LSTM TEST RESULT\")\n",
        "print(f\"Test MSE: {average_mse_lstm:.3f}\")\n",
        "print(f\"Test MAE: {average_mae_lstm:.3f}\")\n",
        "print(f\"Test MAPE: {(average_mape_lstm*100):.2f}%\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"GRU TEST RESULT\")\n",
        "print(f\"Test MSE: {average_mse_gru:.3f}\")\n",
        "print(f\"Test MAE: {average_mae_gru:.3f}\")\n",
        "print(f\"Test MAPE: {(average_mape_gru*100):.2f}%\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"TRANSFORMER TEST RESULT\")\n",
        "print(f\"Test MSE: {average_mse_transformer:.3f}\")\n",
        "print(f\"Test MAE: {average_mae_transformer:.3f}\")\n",
        "print(f\"Test MAPE: {(average_mape_transformer*100):.2f}%\")\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"ENSEMBLE MODEL TEST RESULT\")\n",
        "print(f\"Test MSE: {average_mse_second_layer:.3f}\")\n",
        "print(f\"Test MAE: {average_mae_second_layer:.3f}\")\n",
        "print(f\"Test MAPE: {(average_mape_second_layer*100):.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEezMLvyftXX"
      },
      "source": [
        "# Visualize the results (predicted vs actual)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "9r4ZHX8D1iG7",
        "outputId": "ebe10b32-1cc9-48c0-e7f6-0b06507a915a"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(true_actual, label='Actual Price')\n",
        "plt.plot(predicted_actual, label='Predicted Price (Ensemble Model)')\n",
        "plt.title('Actual vs. Predicted Stock Price (BMRI)')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Stock Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "059a81282cba4b81a5b4908f3f5b5df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0d909e6102248a0984758b927aaf603",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b19bb8a52f74125b27a2d26974b8c48",
            "value": 1
          }
        },
        "1469ea1f14f643bab8e80fdf6460adec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "170f3674827047d0b34f37529e73b9f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "466506bbf67f408f810f3dffce1e82f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "485d711c8a53483481865e7c931bbd20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b19bb8a52f74125b27a2d26974b8c48": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ef3b92b8adc4154b7df0542f469172c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_485d711c8a53483481865e7c931bbd20",
            "placeholder": "​",
            "style": "IPY_MODEL_170f3674827047d0b34f37529e73b9f8",
            "value": " 1/1 [03:53&lt;00:00, 233.25s/it]"
          }
        },
        "b467ac1ced994667b50b1488396f1c9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f67ee036ec6c49208caf7bfbd0eb99cf",
              "IPY_MODEL_059a81282cba4b81a5b4908f3f5b5df8",
              "IPY_MODEL_8ef3b92b8adc4154b7df0542f469172c"
            ],
            "layout": "IPY_MODEL_1469ea1f14f643bab8e80fdf6460adec"
          }
        },
        "c0d909e6102248a0984758b927aaf603": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5463e034da14e2fa2bda9144cd80931": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f67ee036ec6c49208caf7bfbd0eb99cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_466506bbf67f408f810f3dffce1e82f2",
            "placeholder": "​",
            "style": "IPY_MODEL_e5463e034da14e2fa2bda9144cd80931",
            "value": "training and evaluating model: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
